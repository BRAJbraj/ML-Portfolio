# -*- coding: utf-8 -*-
"""KMeansClustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F1P375jkVJG5auGuClfLn5nhDyS_yl_7
"""

! pip install optuna

from sklearn.cluster import KMeans
import optuna
from sklearn.preprocessing import StandardScaler
import pandas as pd
from sklearn.metrics import silhouette_score, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split,cross_val_score

df_original=pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn.csv")
df=df_original.copy()

# Replace whitespace or empty strings with NaN
df["TotalCharges"] = pd.to_numeric(df["TotalCharges"], errors='coerce')

# Drop rows where `TotalCharges` is NaN
df = df.dropna(subset=["TotalCharges"])

df["Churn"]=df["Churn"].map({'No':0, "Yes":1})
# Target
y=df['Churn'].copy()

# Deciding Factors
deciding_factors = ['Partner', 'Dependents', 'OnlineSecurity', 'OnlineBackup',
                    'DeviceProtection', 'TechSupport', 'Contract', 'PaperlessBilling',
                    'PaymentMethod', 'tenure', 'MonthlyCharges', 'TotalCharges']

X=df[deciding_factors].copy()




#print(X["TotalCharges"]).dtype
# Encoding
cat_factors = ['Partner', 'Dependents', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'Contract', 'PaperlessBilling', 'PaymentMethod']
num_factors = ['tenure', 'MonthlyCharges', 'TotalCharges']

# Dropna
X=X.dropna()
# One-hot encode multi-category vars (e.g., Contract, PaymentMethod)
X_encoded = pd.get_dummies(X, columns=cat_factors, drop_first=True) # Encoded features

#print(X_encoded)

# X_test_final_set, y_test_final_set --> they will be used for final testing and model evaluation
X_train,X_test_final_set, y_train, y_test_final_set=train_test_split(X_encoded,y, test_size=0.2, random_state=42, stratify=y,shuffle=True)


# Dividing X_train,y_train into actual training set and development test set
X_train_actual,X_test_dev_set, y_train_actual, y_test_dev_set=train_test_split(X_train,y_train, test_size=0.2, random_state=42, stratify=y_train,shuffle=True)


X_train_actual.isnull().sum()
X_train_actual

from sklearn.preprocessing import StandardScaler


scaler = StandardScaler()

# Fit and transform the training data
X_train_scaled = scaler.fit_transform(X_train_actual)
X_test_dev_set_scaled=scaler.transform(X_test_dev_set) # Use this at development stage
X_test_final_set_scaled=scaler.transform(X_test_final_set)  # Use this for final evaluation

wcss=[]

for i in range(1,20):
  kmeans=KMeans(n_clusters=i,init='k-means++',random_state=50)
  kmeans.fit(X_train_scaled)
  wcss.append(kmeans.inertia_)

# Plotting wcss
plt.plot(range(1,20),wcss)
plt.title('The Elbow Point Graph')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

"""The above grapgh actually doesnot stabalizes so i am thinking of using silhoutte score to know the number of clusters required."""

from sklearn.metrics import silhouette_score
k=[]
sil_score_list=[]
for i in range(2,40):
  kmeans=KMeans(n_clusters=i,init='k-means++',random_state=50)
  kmeans.fit(X_train_scaled)
  labels=kmeans.labels_
  sil_score=silhouette_score(X_train_scaled,labels)
  print(f"Silhouette Score for k={i}: {sil_score}")
  k.append(i)
  sil_score_list.append(sil_score)

k[sil_score_list.index(max(sil_score_list))]

kmeans=KMeans(n_clusters=2,init='k-means++',random_state=50)
  kmeans.fit(X_train_scaled)
  labels=kmeans.labels_
  print(labels)

max(sil_score_list)

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA  # For high-dimensional data

# Reduce to 2D for plotting (if needed)
pca = PCA(n_components=2)
X_2d = pca.fit_transform(X_train_scaled)

# Plot with cluster colors
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=labels, cmap='viridis')  # Colors by label (0=yellow, 1=purple)
plt.title('KMeans Clusters')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar(label='Cluster Label')
plt.show()

from collections import Counter
cluster_counts = Counter(labels)
print(cluster_counts)  # e.g., Counter({0: 75, 1: 65}) if you have 140 points

centroids = kmeans.cluster_centers_
print(centroids)  # Shape: (2, n_features) â€“ one row per cluster

print(f"Inertia: {kmeans.inertia_}")  # Sum of squared distances to centroids

from sklearn.metrics import silhouette_score
score = silhouette_score(X_train_scaled, labels)
print(f"Silhouette Score: {score:.2f}")

test_labels = kmeans.predict(X_test_dev_set_scaled)  # Assigns clusters to test data
print(test_labels)  # Similar array for test points

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA(n_components=4)  # Or set to None to get all components
X_2d = pca.fit_transform(X_train_scaled)

# Explained variance ratio (e.g., [0.4, 0.3] means PC1 explains 40%, PC2 30%)
print("Explained variance ratio:", pca.explained_variance_ratio_)
print("Cumulative variance:", pca.explained_variance_ratio_.cumsum())  # Should be close to 1.0 for good coverage

# Plot scree plot for all components
pca_full = PCA().fit(X_train_scaled)  # Fit without specifying n_components
plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1),
         pca_full.explained_variance_ratio_.cumsum(), marker='o')
plt.title('Scree Plot: Cumulative Explained Variance')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Variance Explained')
plt.show()  # Helps decide optimal n_components (e.g., where it plateaus)

"""The pca analysis is not suitable here because even 7 components capture only 80% of total variance and the total number of columns(features) is 19 (after encoding) so i think it will be very difficult to interpret pca compoents."""

# Explanation of PCA scores
pca.components_.T

X_labels=X_train_actual.copy()
X_labels['Cluster']=labels
X_labels.groupby('Cluster')['tenure'].mean()

X_labels.groupby('Cluster')['MonthlyCharges'].mean()
# Monthly charges is the better seperator than tenure. This also shows that cluster 0 is the group of customers with low monthly charges an
#This also shows that cluster 0 is the group of customers with low monthly charges and cluster 1 is the group of customers with high monthly charges

X_labels[X_labels['Partner_Yes'] == 1].groupby('Cluster').count()

X_labels.groupby('Cluster').size()

percent_customers_with_partners_cluster_0=(X_labels[(X_labels['Partner_Yes'] == 1) & (X_labels['Cluster'] == 0)].shape[0] / X_labels[X_labels['Cluster'] == 0].shape[0]) * 100
print("Precentage of customer with partner in cluster 0:",percent_customers_with_partners_cluster_0)
percent_customers_with_partners_cluster_1=(X_labels[(X_labels['Partner_Yes'] == 1) & (X_labels['Cluster'] == 1)].shape[0] / X_labels[X_labels['Cluster'] == 1].shape[0]) * 100
print("Precentage of customer with partner in cluster 1:",percent_customers_with_partners_cluster_1)

# Both group has same precentage of customers with partners, so we can't distinguish between them on the basis of partners.

percent_customers_with_dependents_cluster_0=(X_labels[(X_labels['Dependents_Yes'] == 1) & (X_labels['Cluster'] == 0)].shape[0] / X_labels[X_labels['Cluster'] == 0].shape[0]) * 100
print("Precentage of customer with dependent in cluster 0:",percent_customers_with_dependents_cluster_0)
X_labels[X_labels['Dependents_Yes'] == 1].groupby(X_labels['Cluster']).count()
percent_customers_with_dependents_cluster_1=(X_labels[(X_labels['Dependents_Yes'] == 1) & (X_labels['Cluster'] == 1)].shape[0] / X_labels[X_labels['Cluster'] == 1].shape[0]) * 100
print("Precentage of customer with dependent in cluster 1:",percent_customers_with_dependents_cluster_1)

percent_customers_with_OnlineSecurity_No_internet_service_cluster_0=(X_labels[(X_labels['OnlineSecurity_No internet service'] == 1) & (X_labels['Cluster'] == 0)].shape[0] / X_labels[X_labels['Cluster'] == 0].shape[0]) * 100
print("Precentage of customer with OnlineSecurity_No internet service in cluster 0:",percent_customers_with_OnlineSecurity_No_internet_service_cluster_0)
#X_labels[X_labels['Dependents_Yes'] == 1].groupby(X_labels['Cluster']).count()
percent_customers_with_OnlineSecurity_No_internet_service_cluster_1=(X_labels[(X_labels['OnlineSecurity_No internet service'] == 1) & (X_labels['Cluster'] == 1)].shape[0] / X_labels[X_labels['Cluster'] == 1].shape[0]) * 100
print("Precentage of customer with OnlineSecurity_No internet service in cluster 1:",percent_customers_with_OnlineSecurity_No_internet_service_cluster_1)

X_labels[X_labels['OnlineSecurity_No internet service'] == 1].groupby(X_labels['Cluster']).count()  # --> all customers from group 0 has no internet service. This is a key distinction between these two groups.

# you have to find if all customers in cluster 1 has internet service or not

X_labels

# Analysis of group/clusters  remaining